{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns            # visualization tool\n",
    "import matplotlib.cm as cm       #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_crossing(temp,threshold):\n",
    "    zc = 0\n",
    "    for i in range(len(temp)-1):\n",
    "        if((temp[i]*temp[i+1]) < 0 and abs(temp[i]-temp[i+1]) >= threshold):\n",
    "            zc += 1\n",
    "    return zc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_one_csv(file_name,k):\n",
    "    data = pd.read_csv('/home/amaya/emp/Realtime-Sign-Language-Translation-to-Speech-DNN/all/User'+str(k)+'/'+file_name)\n",
    "    global classes\n",
    "    \n",
    "    class_ = file_name[0:len(file_name)-14]\n",
    "    \n",
    "    # print(data.head(2))\n",
    "\n",
    "    columns = list(data.columns)[1:]\n",
    "    # features = ['MAV','RMS','VAR','SSI','MAX','MIN']\n",
    "    \n",
    "    vector = []\n",
    "    # global out_columns\n",
    "    \n",
    "    for item in columns:\n",
    "        temp = list(data[item])\n",
    "        # calculating MAV ***\n",
    "        # out_columns.append(item+features[0])\n",
    "        abs_val = list(map(abs,temp))\n",
    "        vector.append(np.mean(abs_val))\n",
    "\n",
    "        # calculating RMS ***\n",
    "        # out_columns.append(item+features[1])\n",
    "        vector.append(np.sqrt(np.mean(np.array(temp)**2)))\n",
    "\n",
    "        # calculate variance ***\n",
    "        # out_columns.append(item+features[2])\n",
    "        x_mean = np.mean(temp)\n",
    "        dif = temp - x_mean\n",
    "        vector.append(np.mean(np.array(dif)**2))\n",
    "\n",
    "        # calculating ssi ***\n",
    "        # out_columns.append(item+features[3])\n",
    "        vector.append(np.sum(np.array(temp)**2))\n",
    "\n",
    "        # calculating max ***\n",
    "        # out_columns.append(item+features[4])\n",
    "        vector.append(max(temp))\n",
    "\n",
    "        # calculating min ***\n",
    "        # out_columns.append(item+features[5])\n",
    "        vector.append(min(temp))\n",
    "        \n",
    "        # calculating zero crossing\n",
    "        if((item[0] == 'E') or (item[0] == 'G')):\n",
    "            vector.append(zero_crossing(temp,1))\n",
    "        elif(item[0] == 'A'):\n",
    "            vector.append(zero_crossing(temp,0.15))\n",
    "        \n",
    "    if class_ in classes:\n",
    "        vector.append(classes.index(class_))\n",
    "    else:\n",
    "        classes.append(class_)\n",
    "        vector.append(classes.index(class_))\n",
    "        \n",
    "    return vector\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(550, 233)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_columns = ['EMG0LMAV', 'EMG0LRMS', 'EMG0LVAR', 'EMG0LSSI', 'EMG0LMAX', 'EMG0LMIN','EMG0ZC', 'EMG1LMAV', 'EMG1LRMS', 'EMG1LVAR', 'EMG1LSSI', 'EMG1LMAX', 'EMG1LMIN','EMG1ZC', 'EMG2LMAV', 'EMG2LRMS', 'EMG2LVAR', 'EMG2LSSI', 'EMG2LMAX', 'EMG2LMIN', 'EMG2ZC','EMG3LMAV', 'EMG3LRMS', 'EMG3LVAR', 'EMG3LSSI', 'EMG3LMAX', 'EMG3LMIN', 'EMG3ZC','EMG4LMAV', 'EMG4LRMS', 'EMG4LVAR', 'EMG4LSSI', 'EMG4LMAX', 'EMG4LMIN', 'EMG4ZC','EMG5LMAV', 'EMG5LRMS', 'EMG5LVAR', 'EMG5LSSI', 'EMG5LMAX', 'EMG5LMIN', 'EMG5ZC','EMG6LMAV','EMG6LRMS', 'EMG6LVAR', 'EMG6LSSI', 'EMG6LMAX', 'EMG6LMIN', 'EMG6ZC','EMG7LMAV', 'EMG7LRMS', 'EMG7LVAR', 'EMG7LSSI', 'EMG7LMAX', 'EMG7LMIN', 'EMG7ZC','AXLMAV', 'AXLRMS', 'AXLVAR', 'AXLSSI', 'AXLMAX', 'AXLMIN', 'AXLZC','AYLMAV', 'AYLRMS', 'AYLVAR', 'AYLSSI', 'AYLMAX', 'AYLMIN', 'AYLZC','AZLMAV', 'AZLRMS', 'AZLVAR', 'AZLSSI', 'AZLMAX', 'AZLMIN', 'AZLZC','GXLMAV', 'GXLRMS', 'GXLVAR', 'GXLSSI', 'GXLMAX', 'GXLMIN', 'GXLZC','GYLMAV', 'GYLRMS', 'GYLVAR', 'GYLSSI', 'GYLMAX', 'GYLMIN', 'GYLZC','GZLMAV', 'GZLRMS', 'GZLVAR', 'GZLSSI', 'GZLMAX', 'GZLMIN', 'GZLZC','ORLMAV', 'ORLRMS', 'ORLVAR', 'ORLSSI', 'ORLMAX', 'ORLMIN', 'OPLMAV', 'OPLRMS', 'OPLVAR', 'OPLSSI', 'OPLMAX', 'OPLMIN', 'OYLMAV', 'OYLRMS', 'OYLVAR', 'OYLSSI', 'OYLMAX', 'OYLMIN', 'EMG0RMAV', 'EMG0RRMS', 'EMG0RVAR', 'EMG0RSSI', 'EMG0RMAX', 'EMG0RMIN', 'EMG0RZC', 'EMG1RMAV', 'EMG1RRMS', 'EMG1RVAR', 'EMG1RSSI', 'EMG1RMAX', 'EMG1RMIN','EMG1RZC', 'EMG2RMAV', 'EMG2RRMS', 'EMG2RVAR', 'EMG2RSSI', 'EMG2RMAX', 'EMG2RMIN','EMG2RZC', 'EMG3RMAV', 'EMG3RRMS', 'EMG3RVAR', 'EMG3RSSI', 'EMG3RMAX', 'EMG3RMIN','EMG3RZC', 'EMG4RMAV', 'EMG4RRMS', 'EMG4RVAR', 'EMG4RSSI', 'EMG4RMAX', 'EMG4RMIN','EMG4RZC', 'EMG5RMAV', 'EMG5RRMS', 'EMG5RVAR', 'EMG5RSSI', 'EMG5RMAX', 'EMG5RMIN','EMG5RZC', 'EMG6RMAV', 'EMG6RRMS', 'EMG6RVAR', 'EMG6RSSI', 'EMG6RMAX', 'EMG6RMIN','EMG6RZC', 'EMG7RMAV', 'EMG7RRMS', 'EMG7RVAR', 'EMG7RSSI', 'EMG7RMAX', 'EMG7RMIN','EMG7RZC', 'AXRMAV', 'AXRRMS', 'AXRVAR', 'AXRSSI', 'AXRMAX', 'AXRMIN','AXRZC', 'AYRMAV', 'AYRRMS', 'AYRVAR', 'AYRSSI', 'AYRMAX', 'AYRMIN','AYRZC', 'AZRMAV', 'AZRRMS', 'AZRVAR', 'AZRSSI', 'AZRMAX', 'AZRMIN','AZRZC', 'GXRMAV', 'GXRRMS', 'GXRVAR', 'GXRSSI', 'GXRMAX', 'GXRMIN','GXRZC', 'GYRMAV', 'GYRRMS', 'GYRVAR', 'GYRSSI', 'GYRMAX', 'GYRMIN','GYRZC', 'GZRMAV', 'GZRRMS', 'GZRVAR', 'GZRSSI', 'GZRMAX', 'GZRMIN','GZRZC', 'ORRMAV', 'ORRRMS', 'ORRVAR', 'ORRSSI', 'ORRMAX', 'ORRMIN', 'OPRMAV', 'OPRRMS', 'OPRVAR', 'OPRSSI', 'OPRMAX', 'OPRMIN', 'OYRMAV', 'OYRRMS', 'OYRVAR', 'OYRSSI', 'OYRMAX', 'OYRMIN','CLASS']\n",
    "# 205 columns (34*6 + 1)\n",
    "output = []\n",
    "classes=[]\n",
    "# print(len(out_columns))\n",
    "\n",
    "# process_one_csv(files[0])\n",
    "for k in range(1,11):\n",
    "    if(k!=5):\n",
    "        files = os.listdir('/home/amaya/emp/Realtime-Sign-Language-Translation-to-Speech-DNN/all/User'+str(k)+'/')\n",
    "    else:\n",
    "        continue\n",
    "    for file in files:\n",
    "        if(file[-1] == 'v'):      # to avoid _DS_STORE files and anyother hiiden files\n",
    "            output.append(process_one_csv(file,k))\n",
    "        \n",
    "processed = pd.DataFrame(output,columns = out_columns)\n",
    "processed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature scaling of data\n",
    "scaler = preprocessing.MaxAbsScaler()   # since the data set is not gaussian\n",
    "scaled_df = scaler.fit_transform(processed[out_columns[:len(out_columns)-1]])\n",
    "X = pd.DataFrame(scaled_df, columns = out_columns[:len(out_columns)-1])\n",
    "y = processed['CLASS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting the dataset\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0823 16:23:43.267088 140430451128064 deprecation.py:506] From /home/amaya/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "# setting up the layers\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(232,input_dim = 232,activation = 'relu'),\n",
    "    keras.layers.Dense(450,activation = 'relu'),\n",
    "    keras.layers.Dense(20,activation = 'softmax')\n",
    "])\n",
    "\n",
    "# compile the model\n",
    "model.compile(optimizer = 'adam', loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "440/440 [==============================] - 0s 285us/sample - loss: 2.8761 - acc: 0.1500\n",
      "Epoch 2/100\n",
      "440/440 [==============================] - 0s 79us/sample - loss: 2.3780 - acc: 0.3500\n",
      "Epoch 3/100\n",
      "440/440 [==============================] - 0s 76us/sample - loss: 1.9216 - acc: 0.4295\n",
      "Epoch 4/100\n",
      "440/440 [==============================] - 0s 78us/sample - loss: 1.6148 - acc: 0.5159\n",
      "Epoch 5/100\n",
      "440/440 [==============================] - 0s 88us/sample - loss: 1.3310 - acc: 0.6182\n",
      "Epoch 6/100\n",
      "440/440 [==============================] - 0s 74us/sample - loss: 1.1600 - acc: 0.6591\n",
      "Epoch 7/100\n",
      "440/440 [==============================] - 0s 76us/sample - loss: 1.0086 - acc: 0.7136\n",
      "Epoch 8/100\n",
      "440/440 [==============================] - 0s 74us/sample - loss: 0.8657 - acc: 0.7795\n",
      "Epoch 9/100\n",
      "440/440 [==============================] - 0s 77us/sample - loss: 0.7349 - acc: 0.8114\n",
      "Epoch 10/100\n",
      "440/440 [==============================] - 0s 75us/sample - loss: 0.6470 - acc: 0.8227\n",
      "Epoch 11/100\n",
      "440/440 [==============================] - 0s 84us/sample - loss: 0.6344 - acc: 0.8455\n",
      "Epoch 12/100\n",
      "440/440 [==============================] - 0s 77us/sample - loss: 0.5350 - acc: 0.8636\n",
      "Epoch 13/100\n",
      "440/440 [==============================] - 0s 74us/sample - loss: 0.5007 - acc: 0.8523\n",
      "Epoch 14/100\n",
      "440/440 [==============================] - 0s 79us/sample - loss: 0.4549 - acc: 0.8864\n",
      "Epoch 15/100\n",
      "440/440 [==============================] - 0s 79us/sample - loss: 0.4150 - acc: 0.8818\n",
      "Epoch 16/100\n",
      "440/440 [==============================] - 0s 86us/sample - loss: 0.3724 - acc: 0.9159\n",
      "Epoch 17/100\n",
      "440/440 [==============================] - 0s 90us/sample - loss: 0.3221 - acc: 0.9091\n",
      "Epoch 18/100\n",
      "440/440 [==============================] - 0s 78us/sample - loss: 0.2850 - acc: 0.9545\n",
      "Epoch 19/100\n",
      "440/440 [==============================] - 0s 71us/sample - loss: 0.2682 - acc: 0.9500\n",
      "Epoch 20/100\n",
      "440/440 [==============================] - 0s 76us/sample - loss: 0.2624 - acc: 0.9386\n",
      "Epoch 21/100\n",
      "440/440 [==============================] - 0s 78us/sample - loss: 0.2406 - acc: 0.9523\n",
      "Epoch 22/100\n",
      "440/440 [==============================] - 0s 83us/sample - loss: 0.2010 - acc: 0.9591\n",
      "Epoch 23/100\n",
      "440/440 [==============================] - 0s 76us/sample - loss: 0.1837 - acc: 0.9705\n",
      "Epoch 24/100\n",
      "440/440 [==============================] - 0s 83us/sample - loss: 0.1610 - acc: 0.9750\n",
      "Epoch 25/100\n",
      "440/440 [==============================] - 0s 79us/sample - loss: 0.1443 - acc: 0.9864\n",
      "Epoch 26/100\n",
      "440/440 [==============================] - 0s 84us/sample - loss: 0.1159 - acc: 0.9955\n",
      "Epoch 27/100\n",
      "440/440 [==============================] - 0s 80us/sample - loss: 0.1098 - acc: 0.9886\n",
      "Epoch 28/100\n",
      "440/440 [==============================] - 0s 80us/sample - loss: 0.1058 - acc: 0.9932\n",
      "Epoch 29/100\n",
      "440/440 [==============================] - 0s 76us/sample - loss: 0.0953 - acc: 0.9977\n",
      "Epoch 30/100\n",
      "440/440 [==============================] - 0s 75us/sample - loss: 0.0911 - acc: 0.9886\n",
      "Epoch 31/100\n",
      "440/440 [==============================] - 0s 81us/sample - loss: 0.0786 - acc: 0.9932\n",
      "Epoch 32/100\n",
      "440/440 [==============================] - 0s 75us/sample - loss: 0.0703 - acc: 0.9977\n",
      "Epoch 33/100\n",
      "440/440 [==============================] - 0s 78us/sample - loss: 0.0806 - acc: 0.9864\n",
      "Epoch 34/100\n",
      "440/440 [==============================] - 0s 86us/sample - loss: 0.0570 - acc: 1.0000\n",
      "Epoch 35/100\n",
      "440/440 [==============================] - 0s 83us/sample - loss: 0.0492 - acc: 1.0000\n",
      "Epoch 36/100\n",
      "440/440 [==============================] - 0s 83us/sample - loss: 0.0449 - acc: 1.0000\n",
      "Epoch 37/100\n",
      "440/440 [==============================] - 0s 89us/sample - loss: 0.0450 - acc: 1.0000\n",
      "Epoch 38/100\n",
      "440/440 [==============================] - 0s 90us/sample - loss: 0.0431 - acc: 1.0000\n",
      "Epoch 39/100\n",
      "440/440 [==============================] - 0s 87us/sample - loss: 0.0379 - acc: 1.0000\n",
      "Epoch 40/100\n",
      "440/440 [==============================] - 0s 78us/sample - loss: 0.0348 - acc: 0.9977\n",
      "Epoch 41/100\n",
      "440/440 [==============================] - 0s 76us/sample - loss: 0.0325 - acc: 1.0000\n",
      "Epoch 42/100\n",
      "440/440 [==============================] - 0s 82us/sample - loss: 0.0315 - acc: 1.0000\n",
      "Epoch 43/100\n",
      "440/440 [==============================] - 0s 76us/sample - loss: 0.0305 - acc: 1.0000\n",
      "Epoch 44/100\n",
      "440/440 [==============================] - 0s 74us/sample - loss: 0.0278 - acc: 1.0000\n",
      "Epoch 45/100\n",
      "440/440 [==============================] - 0s 72us/sample - loss: 0.0232 - acc: 1.0000\n",
      "Epoch 46/100\n",
      "440/440 [==============================] - 0s 79us/sample - loss: 0.0226 - acc: 1.0000\n",
      "Epoch 47/100\n",
      "440/440 [==============================] - 0s 74us/sample - loss: 0.0228 - acc: 1.0000\n",
      "Epoch 48/100\n",
      "440/440 [==============================] - 0s 70us/sample - loss: 0.0205 - acc: 1.0000\n",
      "Epoch 49/100\n",
      "440/440 [==============================] - 0s 74us/sample - loss: 0.0201 - acc: 1.0000\n",
      "Epoch 50/100\n",
      "440/440 [==============================] - 0s 77us/sample - loss: 0.0179 - acc: 1.0000\n",
      "Epoch 51/100\n",
      "440/440 [==============================] - 0s 79us/sample - loss: 0.0179 - acc: 1.0000\n",
      "Epoch 52/100\n",
      "440/440 [==============================] - 0s 75us/sample - loss: 0.0158 - acc: 1.0000\n",
      "Epoch 53/100\n",
      "440/440 [==============================] - 0s 81us/sample - loss: 0.0151 - acc: 1.0000\n",
      "Epoch 54/100\n",
      "440/440 [==============================] - 0s 80us/sample - loss: 0.0141 - acc: 1.0000\n",
      "Epoch 55/100\n",
      "440/440 [==============================] - 0s 75us/sample - loss: 0.0133 - acc: 1.0000\n",
      "Epoch 56/100\n",
      "440/440 [==============================] - 0s 72us/sample - loss: 0.0136 - acc: 1.0000\n",
      "Epoch 57/100\n",
      "440/440 [==============================] - 0s 75us/sample - loss: 0.0139 - acc: 1.0000\n",
      "Epoch 58/100\n",
      "440/440 [==============================] - 0s 83us/sample - loss: 0.0130 - acc: 1.0000\n",
      "Epoch 59/100\n",
      "440/440 [==============================] - 0s 78us/sample - loss: 0.0111 - acc: 1.0000\n",
      "Epoch 60/100\n",
      "440/440 [==============================] - 0s 77us/sample - loss: 0.0109 - acc: 1.0000\n",
      "Epoch 61/100\n",
      "440/440 [==============================] - 0s 77us/sample - loss: 0.0104 - acc: 1.0000\n",
      "Epoch 62/100\n",
      "440/440 [==============================] - 0s 71us/sample - loss: 0.0100 - acc: 1.0000\n",
      "Epoch 63/100\n",
      "440/440 [==============================] - 0s 74us/sample - loss: 0.0094 - acc: 1.0000\n",
      "Epoch 64/100\n",
      "440/440 [==============================] - 0s 78us/sample - loss: 0.0093 - acc: 1.0000\n",
      "Epoch 65/100\n",
      "440/440 [==============================] - 0s 77us/sample - loss: 0.0086 - acc: 1.0000\n",
      "Epoch 66/100\n",
      "440/440 [==============================] - 0s 71us/sample - loss: 0.0089 - acc: 1.0000\n",
      "Epoch 67/100\n",
      "440/440 [==============================] - 0s 79us/sample - loss: 0.0081 - acc: 1.0000\n",
      "Epoch 68/100\n",
      "440/440 [==============================] - 0s 84us/sample - loss: 0.0081 - acc: 1.0000\n",
      "Epoch 69/100\n",
      "440/440 [==============================] - 0s 75us/sample - loss: 0.0076 - acc: 1.0000\n",
      "Epoch 70/100\n",
      "440/440 [==============================] - 0s 77us/sample - loss: 0.0074 - acc: 1.0000\n",
      "Epoch 71/100\n",
      "440/440 [==============================] - 0s 88us/sample - loss: 0.0071 - acc: 1.0000\n",
      "Epoch 72/100\n",
      "440/440 [==============================] - 0s 78us/sample - loss: 0.0068 - acc: 1.0000\n",
      "Epoch 73/100\n",
      "440/440 [==============================] - 0s 75us/sample - loss: 0.0065 - acc: 1.0000\n",
      "Epoch 74/100\n",
      "440/440 [==============================] - 0s 79us/sample - loss: 0.0062 - acc: 1.0000\n",
      "Epoch 75/100\n",
      "440/440 [==============================] - 0s 75us/sample - loss: 0.0060 - acc: 1.0000\n",
      "Epoch 76/100\n",
      "440/440 [==============================] - 0s 73us/sample - loss: 0.0058 - acc: 1.0000\n",
      "Epoch 77/100\n",
      "440/440 [==============================] - 0s 76us/sample - loss: 0.0057 - acc: 1.0000\n",
      "Epoch 78/100\n",
      "440/440 [==============================] - 0s 85us/sample - loss: 0.0054 - acc: 1.0000\n",
      "Epoch 79/100\n",
      "440/440 [==============================] - 0s 87us/sample - loss: 0.0055 - acc: 1.0000\n",
      "Epoch 80/100\n",
      "440/440 [==============================] - 0s 87us/sample - loss: 0.0053 - acc: 1.0000\n",
      "Epoch 81/100\n",
      "440/440 [==============================] - 0s 79us/sample - loss: 0.0049 - acc: 1.0000\n",
      "Epoch 82/100\n",
      "440/440 [==============================] - 0s 75us/sample - loss: 0.0051 - acc: 1.0000\n",
      "Epoch 83/100\n",
      "440/440 [==============================] - 0s 88us/sample - loss: 0.0049 - acc: 1.0000\n",
      "Epoch 84/100\n",
      "440/440 [==============================] - 0s 84us/sample - loss: 0.0046 - acc: 1.0000\n",
      "Epoch 85/100\n",
      "440/440 [==============================] - 0s 91us/sample - loss: 0.0044 - acc: 1.0000\n",
      "Epoch 86/100\n",
      "440/440 [==============================] - 0s 82us/sample - loss: 0.0042 - acc: 1.0000\n",
      "Epoch 87/100\n",
      "440/440 [==============================] - 0s 79us/sample - loss: 0.0040 - acc: 1.0000\n",
      "Epoch 88/100\n",
      "440/440 [==============================] - 0s 73us/sample - loss: 0.0040 - acc: 1.0000\n",
      "Epoch 89/100\n",
      "440/440 [==============================] - 0s 75us/sample - loss: 0.0038 - acc: 1.0000\n",
      "Epoch 90/100\n",
      "440/440 [==============================] - 0s 74us/sample - loss: 0.0037 - acc: 1.0000\n",
      "Epoch 91/100\n",
      "440/440 [==============================] - 0s 73us/sample - loss: 0.0037 - acc: 1.0000\n",
      "Epoch 92/100\n",
      "440/440 [==============================] - 0s 81us/sample - loss: 0.0037 - acc: 1.0000\n",
      "Epoch 93/100\n",
      "440/440 [==============================] - 0s 88us/sample - loss: 0.0036 - acc: 1.0000\n",
      "Epoch 94/100\n",
      "440/440 [==============================] - 0s 75us/sample - loss: 0.0034 - acc: 1.0000\n",
      "Epoch 95/100\n",
      "440/440 [==============================] - 0s 76us/sample - loss: 0.0032 - acc: 1.0000\n",
      "Epoch 96/100\n",
      "440/440 [==============================] - 0s 81us/sample - loss: 0.0033 - acc: 1.0000\n",
      "Epoch 97/100\n",
      "440/440 [==============================] - 0s 86us/sample - loss: 0.0031 - acc: 1.0000\n",
      "Epoch 98/100\n",
      "440/440 [==============================] - 0s 76us/sample - loss: 0.0030 - acc: 1.0000\n",
      "Epoch 99/100\n",
      "440/440 [==============================] - 0s 74us/sample - loss: 0.0030 - acc: 1.0000\n",
      "Epoch 100/100\n",
      "440/440 [==============================] - 0s 75us/sample - loss: 0.0029 - acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fb7f7ec6630>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training the model\n",
    "model.fit(X_train,y_train,epochs = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110/110 [==============================] - 0s 387us/sample - loss: 0.3310 - acc: 0.9182\n",
      "0.91818184\n"
     ]
    }
   ],
   "source": [
    "# evaluate accuracy\n",
    "test_loss,test_acc = model.evaluate(X_test,y_test)\n",
    "print(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n",
      "11\n",
      "13\n",
      "0\n",
      "9\n",
      "16\n",
      "17\n",
      "18\n",
      "1\n",
      "17\n",
      "0\n",
      "18\n",
      "19\n",
      "11\n",
      "4\n",
      "17\n",
      "0\n",
      "7\n",
      "2\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# predict\n",
    "predictions = model.predict(X_test)\n",
    "for i in range(20):\n",
    "    print(np.argmax(predictions[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n",
      "11\n",
      "13\n",
      "0\n",
      "9\n",
      "16\n",
      "17\n",
      "18\n",
      "1\n",
      "17\n",
      "15\n",
      "18\n",
      "19\n",
      "11\n",
      "4\n",
      "17\n",
      "0\n",
      "7\n",
      "2\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    print(list(y_test)[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
